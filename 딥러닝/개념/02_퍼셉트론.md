# 퍼셉트론

퍼셉트론은 프랑크 로젠블라트(Frank Rosenblatt)가 1957년에 고안한 알고리즘

신경망의 기원이 되는 알고리즘이다.



퍼셉트론은 다수의 신호를 입력으로 받아 하나의 신호를 출력한다.

입력과 가중치의 가중합으로 만들어진 신호가 특정 값(임계치)을 넘어서면 1로 출력, 넘지 못할 때 0으로 출력되는 알고리즘이다.

$ y = \begin{cases}0 &(w_1x_1 + w_2x_2 \le \theta) \\ 1 & (w_1x_1 + w_2x_2 > \theta) \end{cases} $

가중치는 신호가 얼마나 영향을 끼치는지에 대한 값이다.



## 논리 게이트

### AND Gate

| x1   | x2   | y    |
| ---- | ---- | ---- |
| 0    | 0    | 0    |
| 1    | 0    | 0    |
| 0    | 1    | 0    |
| 1    | 1    | 1    |

AND 게이트는 입력이 둘 다 1일 때만 1을 출력하고 나머지는 0을 출력한다.



### NAND Gate

| x1   | x2   | y    |
| ---- | ---- | ---- |
| 0    | 0    | 1    |
| 1    | 0    | 1    |
| 0    | 1    | 1    |
| 1    | 1    | 0    |

NAND 게이트는 입력이 둘다 1일 때만 0을 출력하고 나머지는 1을 출력한다.



### OR Gate

| x1   | x2   | y    |
| ---- | ---- | ---- |
| 0    | 0    | 0    |
| 1    | 0    | 1    |
| 0    | 1    | 1    |
| 1    | 1    | 1    |





## 편향 도입

기존 퍼셉트론식에 임계치($\theta$) 대신 편향을 도입해보자. (theta대신 -b를 적용)

$$ y = \begin{cases}0 &(w_1x_1 + w_2x_2 + b \le 0) \\ 1 & (w_1x_1 + w_2x_2 + b > 0) \end{cases} $$



$w1, w2$는 각 입력 신호가 결과에 주는 영향력(중요도)을 조절하는 매개변수고, 편향은 뉴런이 얼마나 쉽게 활성화하느냐를 조정해 주는 매개변수다.

가중합이 편향을 넘지 못하면 출력이 0, 편향을 넘어서면 1로 출력되는 것이다.

편의상 weight와 biase를 그냥 가중치라고 한다.



## 퍼셉트론의 한계

XOR문제(배타적 논리합)를 풀지못한다. 이는 AI winter로 이어진다. 



### XOR 문제

| x1   | x2   | y    |
| ---- | ---- | ---- |
| 0    | 0    | 0    |
| 1    | 0    | 1    |
| 0    | 1    | 1    |
| 1    | 1    | 0    |

<img src="C:\Users\Jay\Desktop\딥러닝내용정리\data\src\xor.jpg" alt="xor" style="zoom:120%;" align="left"/>

직선 하나(단층)로는 다음의 영역을 나누는게 불가능하다는 문제가 생겼다.
(Single Layer Perceptron의 한계)

선형(직선)으로는 다음의 영역을 분리할 수 없지만 비선형(곡선)을 이용하면 다음의 영역을 둘로 나눌수 있게 된다.



## 다층 퍼셉트론

단층 퍼셉트론(SLP)로는 XOR을 표현할 수 없었다. 

층을 여러개 쌓아 만드는 다층 퍼셉트론(Multi Layer Perceptron)으로는 가능하다.

예로 2층의 퍼셉트론을 구현한다 할 때 `OR`과 `NAND` 게이트를 조합하면 XOR을 표현할 수 있다.

s1은 NAND, s2는 OR

| x1   | x2   | s1   | s2   | y    |
| ---- | ---- | ---- | ---- | ---- |
| 0    | 0    | 1    | 0    | 0    |
| 1    | 0    | 1    | 1    | 1    |
| 0    | 1    | 1    | 1    | 1    |
| 1    | 1    | 0    | 1    | 0    |





## Regression MLP

| 하이퍼파라미터       | 일반적인 값                                                  |
| -------------------- | ------------------------------------------------------------ |
| 입력 뉴런 수         | 특성마다 하나                                                |
| 은닉층 수            | 문제에 따라 다름, 일반적으로 1에서 5 사이                    |
| 은닉층의 뉴런 수     | 문제에 따라 다름, 일반적으로 10에서 100사이                  |
| 출력 뉴런 수         | 예측 차원마다 하나                                           |
| 은닉층의 활성화 함수 | ReLU                                                         |
| 출력층의 활성화 함수 | 항등함수, 또는 출력이 양수일 땐 ReLU/Softplus, 출력 범위 제한일 시 logistic/tanh사용 |
| 손실함수             | MSE나 MAE/Huber                                              |





## Classification MLP

| 하이퍼파라미터       | 이진분류        | 다중 레이블 분류 | 다중 분류       |
| -------------------- | --------------- | ---------------- | --------------- |
| 입력층과 은닉층      | 회귀와 동일     | 회귀와 동일      | 회귀와 동일     |
| 출력 뉴런 수         | 1개             | 레이블마다 1개   | 클래스마다 1개  |
| 출력층의 활성화 함수 | 로지스틱 함수   | 로지스틱 함수    | 소프트맥스 함수 |
| 손실 함수            | 크로스 엔트로피 | 크로스 엔트로피  | 크로스 엔트로피 |













