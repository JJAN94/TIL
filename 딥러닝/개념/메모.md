# 메모

은닉층의 연결 가중치를 랜덤하게 초기화하는 것이 중요하다. 그렇지 않으면 훈련이 실패할 것이다. 





### 은닉층 개수

심층 신경망이 얖은 신경망보다 파라미터 효율성(parameter efficiency)가 훨씬 좋다.

심층산경망은 복잡한 함수를 모델링하는 데 얕은 신경망보다 훨씬 적은 수의 뉴런을 사용하므로 동일한 양의 훈련 데이터에서 더 높은 성능을 낼 수 있다.

새로운 신경망에서 처음 몇 개 층의 가중치와 편향을 난수로 초기화하는 대신 첫 번째 신경망의 층에 있는 가중치와 편향값을 초기화할 수 있다. 이를 전이학습(transfer learning)이라고 한다.



### 은닉층 뉴런 개수

은닉층의 구성 방식은 일반적으로 각 층의 뉴런을 점점 줄여서 깔때기처럼 구성한다. 저수준의 많은 특성이 고수준의 적은 특성으로 합쳐질 수 있기 때문이다. 그러나 요즘은 꼭 그렇지많도 않다. 층의 개수와 마찬가지로 뉴런의 수를 필요한 것보다 더 많이 만들고, 과대적합되지 않도록 조기 종료나 규제 기법을 사용한다. 이를 `스트레치 팬츠(stretch pants)`방식이라 한다.

다른 한편, 한층의 뉴런 수가 너무 적으면 입력에 있는 유용한 정보를 모두 유지하기 위한 충분한 표현 능력을 가지지 못한다.





### 학습률

가장 중요한 하이퍼파라미터





### 옵티마이저

고전적인 평범한 미니배치 경사 하강법보다 더 좋은 옵티마이저를 선택하는 것이 중요





### 배치 크기

배치 크기는 모델 성능과 훈련 시간에 큰 영향을 미친다. 일반적으로 배치크기가 커지면 학습시간은 줄어드나 성능도 줄어들 수도?

학습률을 조절하는게 최고의 하이퍼파라미터 조절법이다. 특히, 이는 배치 크기에 영향을 많이 받는다. 그러므로 다른 하이퍼파라미터를 조절하면 학습률도 반드시 튜닝해줘야 한다.



### 활성화함수

일반적으로 ReLU가 모든 은닉층에 좋은 기본값이다.



### 반복횟수

보통 반복횟수는 건들 필요가 없다. 단, 조기종료를 사용할 수 있다.





# 핸즈온에서 추천해주는 논문

신경망 하이퍼파라미터 튜닝에 관한 좋은 모법 사례 소개 -레슬리 스미스(Leslie Smith) 2018

Leslie N.Smith "A Disciplined Approach to Neural Network Hyper-Parameters:Part1"

https://homl.info/1cycle







# 궁금증

학습률 예열(warming up)이 뭐지. 11장에서 다룬다는데? 작은 학습률로 훈련을 시작해서 점점 커진다고..?