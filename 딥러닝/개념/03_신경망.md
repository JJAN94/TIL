# 신경망

퍼셉트론의 가중치와 편향은 사람이 직접 지정해줘야 했다.

그러나 신경망은 가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습하는 능력이 있다.



## 신경망의 구조

<img src="C:\Users\Jay\Desktop\딥러닝내용정리\data\src\layer.png" alt="layer" style="zoom:110%;" align="left"/>



위의 그래프는 신경망의 예이다. 신경망의 베이스가 되는 것은 퍼셉트론이다.

Input Layer(입력층), Hidden Layer(은닉층), 출력층(Output Layer)으로 구성되어 있다.



## 신경망 학습의 매커니즘

신경망의 학습은 크게 2가지로 구성되어있다.

1. Forward Propagation (순전파)
2. Back Propagation (역전파)

이 2가지 방법에 대해 알아보자.



### Forward Propagation

#### Parametric Functions

딥러닝 함수의 기초는 Parametric Function이다. 이는 `입력`과 `파라미터`로 표현된다.

$$x \in \mathbb{R} \quad \rightarrow \quad f(x; \theta) \quad \rightarrow \quad z \in \mathbb{R}$$

실수의 입력 데이터 $x$가 함수 $f(x;\theta)$에 들어가 $z$를 출력한다. 



#### Input Data

입력 데이터는 컬럼 형태로 들어온다. (편의상 전치를 해줘서 표기해주자)

$$ \vec{x}^T = (x_1\ x_2\ \cdots\ x_{l_I} )$$

$$ (\vec{x}^{(1)})^T = (x_1^{(1)}\ x_2^{(1)}\ \cdots\ x_{l_I}^{(1)} )$$

$$ (\vec{x}^{(2)})^T = (x_1^{(2)}\ x_2^{(2)}\ \cdots\ x_{l_I}^{(2)} )$$

$$ \vdots $$

$$ (\vec{x}^{(N)})^T = (x_1^{(N)}\ x_2^{(N)}\ \cdots\ x_{l_I}^{(N)} )$$



윗 첨자 $1, 2, \cdots, N$은 각각의 feature를 의미한다.
아래첨자 $1, 2, \cdots, l_I$는 feature의 길이를 의미 ($l_I$ : length of input)



그래서 각 feature들이 행렬(tensor)를 이뤄서 입력으로 쓰이게 된다.

$$ X^T = \begin{bmatrix} x^{(1)} \\ x^{(2)} \\ \vdots \\ x^{(N)} \end{bmatrix} \in \mathbb{R}^{N \times 1}$$



$$ X^T = \begin{bmatrix} \leftarrow (\vec{x}^{(1)})^T \rightarrow \\ \leftarrow (\vec{x}^{(2)})^T \rightarrow \\ \vdots \\ \leftarrow (\vec{x}^{(N)})^T \rightarrow \\ \end{bmatrix} = \begin{bmatrix} x_1^{(1)} & x_2^{(1)} & \cdots & x_{n_I}^{(1)} \\ x_1^{(2)} & x_2^{(2)} & \cdots & x_{n_I}^{(2)}\\ \vdots & \vdots & \ddots & \vdots \\ x_1^{(N)} &  x_2^{(N)} & \cdots& x_{n_I}^{(N)} \end{bmatrix} \in \mathbb{R}^{N \times l_I}$$





#### Affine Transformation

이제 입력 데이터는 살펴봤으니 가중합과 편향을 더해주는 연산을 살펴보자. (퍼셉트론과 같다)

- Weighted Sum(가중합)
  - $z = W_1X_1 + W_2X_2 + \cdots W_nX_n = (W)^T\cdot X = X^T\cdot W$
  - 주로 $X^T \cdot W$ 을 쓴다.
- Affine Transformation
  - 기존 가중합에 편향을 주자.
  - $z = W_1X_1 + W_2X_2 + \cdots W_nX_n = X^T\cdot W + \vec{b}$



#### Activation Function

이제 순전파의 마지막 단계인 활성화 함수에 대해 살펴보자.

다음은 대표적인 활성화 함수이다.



<img src="C:\Users\Jay\Desktop\딥러닝내용정리\data\src\activation.jpg" alt="activation" style="zoom:100%;" align="left" />

- Sigmoid
  - $g(x) = \sigma(x) = \frac{1}{1+exp(-x)}$
- Tanh
  - $g(x) = tanh(x) = \frac{exp(x) - exp(-x)}{exp(x) + exp(-x)}$
- ReLU
  - $g(x) = ReLU(x) = max(0, x)$



다음의 활성화 함수는 비선형을 갖는다. 

신경망에서는 선형 활성화함수를 쓰면 안된다. 선형을 쓸 경우 신경망의 층을 깊게 하는 의미가 없어지기 때문이다. 
(선형에 선형을 더해봤자 다시 선형성을 갖는다. 아무리 층을 많이 쌓아도 하나의 층과 동일해진다.)



정리를 해보면 순전파 과정에선

$ X^T \in \mathbb{R}^{N \times l_I}\ \rightarrow\ z = f(X^T; W, \vec{b})\ \rightarrow\ a = Activation(z)$







#### Sigmoid and Softmax

- Odds function
  - $o = \frac{p}{1-p}$
- Logit function
  - $l = log(\frac{p}{1-p})$
  - Odds 함수에 로그를 씌우면 Logit 함수가 된다.
  - 즉, p(확률)을 넣어주면 logit이 나온다.
- Sigmoid function
  - Logit 함수의 역함수
  - $p = \sigma(l) = \frac{1}{1+exp(-l)}$
  - 즉, logit을 넣어주면 p(확률)이 나온다.

<img src="C:\Users\Jay\Desktop\딥러닝내용정리\data\src\logit.jpg" alt="logit" style="zoom:100%;" align="left"/>





- Softmax
  - Multi-class classification
  
  - $p_i = P(C_i), 1 \le i \le K$
  
  - $p_i = \frac{e^{l_i}}{ \sum_{k=1}^{K} e^{l_k}}$
  
  - 각 클래스에 맞는 값을 확률로서 표현해준다. 분류해야할 클래스가 10개면 10개에 대한 확률을 나타내 가장 높은 확률이 그 클래스라 예측한다.
  
    
    
#### Loss Function

- Regression
  - MSE(Mean Squared Error)
  - $J = \frac{1}{N} \sum_{i=1}^{N} (y^{(i)} - \hat{y}^{i})^2$
- Binary Classification
  - Binary Cross Entropy
  - $H_b(y, \hat{y}) = - [ylog(\hat{y}) + (1-y)log(1-\hat{y})]$
  - $y$는 0 과 1 두가지를 갖는다.
  - $y = 0$이면 $H_b = -log(1-\hat{y})$를 갖는다. 이때 $\hat{y}$가 0에 근접할수록 $H_b = 0$, $\hat{y}$가 1에 근접할수록 Loss가 무한으로 커진다.
    - 즉, $y=0$일때 예측값이 0에 근접할수록 Loss는 준다.
  - $y=1$이면 $H_b = -log(\hat{y})$를 갖는다. $\hat{y}$가 0에 근접할 수록 Loss는 무한, 1에 근접할수록 Loss는 0에 가까워진다. 
    - 즉, $y=1$일때 예측값이 1에 근접할 수록 Loss는 준다.
  - 배치로 확장하면 Loss(jacobian)은
    - $J = H_b(Y, \hat{Y}) = -\frac{1}{N} \sum_{i=1}^N [y^{(i)}log(\hat{y}^{(i)})+(1-y^{(i)})log(1-\hat{y}^{(i)})]$
- Multi-Class Classification
  - Categorical Cross Entropy (일반적인 categorical C.E는 One-Hot encoding된 $\hat{y}$에 써야한다.)
  - $H(Y^T, \hat{Y}^T) = \frac{1}{N} \sum_{i=1}^N H(\vec{y}^{(i)}, \vec{\hat{y}}^{(i)}) = -\frac{1}{N} \sum_{i=1}^N \sum_{j=1}^K {y_j}^{(i)}log(\hat{y_j}^{(i)})$
  - 원-핫 인코딩 된 예측값에 따라서 정답이 1, 오답이0이므로 sum을 해도 binary cross entropy와 같은 효과를 갖는다.
  - Sparse Categorical Cross Entropy : One-Hot encoding되지 않을 때 쓴다.

   





### Back Propagation





