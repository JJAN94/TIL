# Perceptron

### 퍼셉트론?

<img src=".\src\perceptron1.png" alt="perceptron;" aling="left" style="zoom:70%;" align="left"/>



출력 $F(w_0+w_1*x_1+w_2*x_2+... + w_n*x_n) $

여기서 $F$는 Activation Function

각 입력값들에 weigths를 곱해서 가중합(weighted sum)을 해 activation함수에 입력을 해 일정 값(활성화값)을 넘어서면 출력한다.

Activation Function = Step Function, ReLU, Sigmoid....

여기서 <font color='red'>가중치(weight)</font>를 구하는게 핵심.
최초에 가중치를 설정(랜덤이나 가중치 초기화값)해서 입력 feature들로 예측값을 계산한다. 예측값과 실제값의 오차(error, residual)를 구해 오차를 줄이는 방향으로 계속해서 가중치(weight)를 수정해 나간다.



### 회귀(Regression)

회귀는 여러 개의 독립변수(feature)와 한 개의 종속변수 간의 상관관계를 모델링하는 기법을 통칭

$ \hat{y} = W_1X_1 + W_2X_2 + .... W_nX_n$

에서 $W_1, W_2,... ,W_N$는 회귀 계수(regression coefficients)

머신러닝 회귀 예측의 핵심은 <font color='red'>주어진 feature와 label 데이터 기반에서 학습을 통해 최적의 회귀 계수를 찾는 것</font>>

그렇다면 최적의 회귀 계수를 찾는다는 것은 무엇일까?

**전체 데이터의 잔차(오류 값) 합이 최소가 되는 모델을 만드는 것**

<img src="C:\Users\JAY\Desktop\github\TIL\Deep_Learning\딥러닝개념\src\reg.png" alt="reg" style="zoom:80%;" align='left'/>

빨간 선이 예측해낸 모델이고, 녹색점은 각각의 label (y)값. 예측값과 실제값의 잔차합을 최소로 하는 가중치를 찾아 다음과 같은 선을 자~알 그리는 것이 회귀 모형 



### 잔차를 나타내는 공식

1. **RSS** (Residual Sum of Square)
   - 오류 값의 제곱을 구해서 더하는 방식. (제곱을 하는 이유는 보통 미분을 편하게 하기 위해서)
   - $$RSS(w_0, w_1) = \sum^{N}_{i=1}(y_i - (w_0+w_1x_i))^2$$
   - <font color='red'>w값(회귀계수)가 중심!!</font>



2. **MSE** (Mean Square Error)

   - $$MSE(w_0, w_1) = \frac{1}{N}\sum^{N}_{i=1}(y_i - (w_0+w_1x_i))^2$$
   - RSS를 학습 데이터의 건수로 나눈 것
   - 다른 이름으로는 비용함수(cost function), 손실함수(loss function), 목적함수(object function)이라고 한다.
   - 최종적으로 MSE를 최소화 하는(더 이상 감소하지 않는) 회귀계수를 가진 모델을 구하는 것.

   

