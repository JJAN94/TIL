# 딥러닝 학습/추론 그리고 하드웨어

## 학습과 추론 개념

딥러닝 **학습(training)**은 축적된 데이터를 바탕으로 신경망 내의 weight를 업데이트 하여 모델을 만드는 과정이다.

**추론(inference)**은 만들어진 모델을 활용해 주어진 input에 대한 해석 결과를 내놓는 단계이다.



학습 단계에선 루프를 forward, backward로 돌며 계속적인(epoch와 iteration) 업데이트를 한다. 반면 추론은 단 한번의 forward propagation을 수행 후 종료된다.



## 하드웨어 성능 지표

딥러닝 수행 시 하드웨어서 필요한 능력은 무엇일까? 

학습적인 측면에선 **throughput(처리량)**이다. 

throughput은 **단위 시간당 처리하는 연산량**을 뜻한다. 

딥러닝 학습은 얼마나 많은 데이터를 바탕으로 수행되었냐에 따라 모델의 정확도가 결정되기 때문에 상당히 많은 데이터가 필요하다. 예로, CNN 분류 작업시 한번 학습하는데도 학습 파라미터수가 수천만에서 억, 그 이상이 되기도 한다. 많은 데이터와 이런 수많은 파라미터들을 계산하기 위해서는 높은 troughput이 요구된다.



학습과는 달리 추론용 하드웨어에서 요구되는 능력은 **Latency**이다.

Latency는 딥러닝 추론 요구의 도착에서부터 수행완료까지 걸리는 시간이다. 영상 인식과 같은 딥러닝 처리도 사용자 입장에서 보면 서비스이다. 필요 이상의 성능과 함께 수행 시간은 중요하다. 





학습용 하드웨어는 높은 throughput을 달성하도록 연산량이 큰 가속기(GPU, ASIC)와 이를 뒷받침하는 고성능 메모리(HBM)으로 구성된다. 딥러닝 가속기가 1초당 연산할 수 있는 양을 주로 TFLOPS라고 한다.

학습용 연산기는 16bit 이상 부동 소수점을 주로 지원한다. 부동 소수점 표현에는 Double Precision Floating, Single Precision Floating이 있는데 하나의 수를 몇 bit로 표현하느냐에 따라 달라진다. 많은 bit을 사용하면 더 정확한 연산이 가능하지만 그만큼 많은 메모리가 필요하고 연산에도 부담이 간다. 초반 딥러닝계에서는 학습 정확도를 위해 32, 64bit을 사용했으나 현재는 구글이 제안한 16bit 방식의 bfloat을 많이 채택해 쓰고 있다. 메모리 관점에서는 이러한 대량 연산을 지원하기 위해 대역폭이 가장 큰 메모리 인터페이스인 HBM이 활용된다.

> HBM이란 High Bandwidth Memory, 고대역 메모리라 한다. 고성능 그래픽스 가속기와 네트워크 장치와 결합하기 위해 사용되는 3D 스택 방식의 DRAM을 위한 고성능 RAM 인터페이스다.



학습용 하드웨어의 경우 가속기들간의 고대역, 고속 통신도 중요하다. 학습의 경우 처리해야할 데이터가 많고 이를 반복해서 많이 수행해야 하기 때문에 여러 가속기에 분배하여 처리한다. 일을 분배하는 방법은 Data-level parallel 방식과 Model-level parallel 방식 두 가지가 있다.

Data-level parallel은 학습해야 할 데이터를 각 연산기에 나누는 방식이다. 예를 들어 1000개 데이터를 100개씩 10개로 나누어 학습 후 연산 결과를 취합하는 방식이다.

Model-level parallel은 딥러닝 모델을 여러 부분으로 나누어 처리하는 방법이다. 50개의 Layer가 있는 딥러닝 모델을 가정하면 10개의 Layer씩 나누어 10개의 가속기가 처리하는 방법이다. 각 가속기가 처리한 결과를 다음 Layer가 있는 가속기로 넘겨주는 방식이다.

어떤 방식을 사용하든 가속기간의 연산 결과 공유를 위한 연결이 필요하게 되고 이로 인해, NVIDIA에선 NVLink 등을 통해 고대역 연결 시스템을 도입해 학습에서의 통신 문제를 해결한다.



추론용 하드웨어는 Latency 단축을 위해 학습용 보다는 좀 더 작은 단위의 가속기를 활용한다. 서버 등에서는 동일한 추론 요구를 모아 한번에 처리하는 Batch 방식을 사용한다. Batch 방식을 사용하면 연산 효율성이 높아지는 장점이 있다. 예를 들어 10개의 영상인식 처리를 Batch로 처리한다면, 메모리에서 딥러닝 모델을 10번이 아닌 1번만 불러와도 되며, 동일 연산도 10회에서 1회로 감소한다.

물론 처리해야할 입력값이 커지기 때문에 단위 연산량이 증가하긴 하지만 전체적인 효율성 증가와 비교하면 부담은 미미하다. 하지만 Batch가 너무 커지면 Batch를 모으는데 시간이 오래 걸리기 때문에 처음 도착한 서비스 요청의 시간이 QoS를 달성하지 못할 수 있다. 따라서 추론에서는 무작정 Batch를 늘릴 수 없으며 이로 인해 학습보다 Batch size가 작고 가속기도 이에 맞춰 Size가 작아진다. 

또한 추론에서는 8/16bit 고정 소수점을 활용한다. 처음에는 학습과 동일하게 부동 소수점을 활용했지만 Back Propagation이 없는 추론의 경우 고정 소수점을 활용해도 추론 결과(정확성)에 큰 차이가 없다는 연구 결과들이 나오며 8/16bit고정 소수점 사용이 일반화 되었다.



추론용 하드웨어는 GDDR이나 DDR을 사용한다. 추론용 하드웨어는 동일 시간에 처리할 수 있는 연산량이 학습용에 비해 작기 때문에 메모리로부터 공급받아야 할 데이터의 양도 적다. 따라서 HBM보다 대역폭(Bandwidth)가 작은 GDDR이나 DDR을 사용한다. HBM을 사용해도 상관없지만 필요하지도 않게 비싼 메모리 인터페이스를 활용하는 것은 비용 부담이 클 테니까



## memo

- 학습용과 추론용 하드웨어를 다르게 쓰는지 알게 되었다.
- bit 표현에 대해 더 공부해보자.
- 그냥 단순하게 공부할 땐 float32로 학습을 했는데, inference할 때 어떻게 해야할지 더 찾아보고 공부 해보자.
- 좋은 GPU 사고싶다...



