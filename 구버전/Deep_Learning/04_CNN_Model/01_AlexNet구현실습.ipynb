{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a144344c-ed7d-4e5e-a861-2c6650ef4134",
   "metadata": {},
   "source": [
    "# CNN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196e8d19-0393-463a-aeca-b3e3c9f9a352",
   "metadata": {},
   "source": [
    "<img src=\"../img/cnn001.jpg\" alt=\"CNN_example\" style=\"zoom:50%;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10d588e-4032-4077-896a-0b50cb726031",
   "metadata": {},
   "source": [
    "- Feature Extractor와 Classification Layer 영역으로 구성\n",
    "- 연속적인 CNN 연산을 순차적으로 수행하면서 일련의 Feature Map을 생성\n",
    "- 순차적으로 생성된 Feature map의 크기 (높이x너비)는 줄어들지만 채널(깊이)는 계속 증가\n",
    "- 더 깊은 영역에 있는 Network이 더 복잡한 Feature 정보를 반영하면서 CNN 깊이를 증가시키는 방향으로 아키텍처를 발전 시킴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c513774-0fdd-4539-9dab-48de8b7d604a",
   "metadata": {},
   "source": [
    "# AlexNet 개요 및 특징\n",
    "- Activation 함수로 ReLU 함수를 첫 사용\n",
    "- MaxPooling 으로 Pooling 적용 및 Overlapping Pooling 적용\n",
    "- Local Response Normalization(LRN) 사용\n",
    "- Overfitting을 개선하기 위해서 Drop out Layer와 Weight의 Decay 기법 적용\n",
    "- Data Augmentation 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bc254a-8940-483d-af37-0e0b15c5546c",
   "metadata": {},
   "source": [
    "- AlexNet은 8개의 레이어로 구성\n",
    "- 5 Conv layer + 3 FCL로 구성\n",
    "\n",
    "<img src=\"../img/alex.jpg\" alt=\"AlexNet\" style=\"zoom:30%;\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5d23dc-069e-45c6-a780-5ee2eab76735",
   "metadata": {},
   "source": [
    "- 11x11, 5x5 사이즈의 큰 사이지의 Kernel적용, 이후 3x3 Kernel을 3번 이어서 적용\n",
    "- Receptive Field가 큰 사이즈를 초기 Feature map에 적용하는 것이 보다 많은 feature 정보를 만드는데 효율적\n",
    "- 많은 weight parameter 개수로 인하여 컴퓨팅 파워가 많이 소요됨. 이를 극복하기 위해 gpu를 병렬로 이용하게 모델을 병렬화함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b7c4b28-7b30-4210-ac55-4682dfb1dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Dropout, Flatten, Activation, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67434dce-5a3c-4336-b7d7-b170c5b0b9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 227, 227, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 55, 55, 96)        34944     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 55, 55, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 55, 55, 96)        384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 27, 27, 256)       614656    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 27, 27, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 27, 27, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 13, 13, 384)       885120    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 13, 13, 384)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 13, 13, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 13, 13, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 13, 13, 384)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 13, 13, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 13, 13, 256)       884992    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 13, 13, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4096)              37752832  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 58,327,818\n",
      "Trainable params: 58,325,066\n",
      "Non-trainable params: 2,752\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input shape, classes 개수, kernel_regularizer등을 인자로 가져감. \n",
    "# cifar 10으로 테스트 할것이기 때문에 n_classes=10\n",
    "def create_alexnet(in_shape=(227, 227, 3), n_classes=10, kernel_regular=None):\n",
    "    # 첫번째 CNN->ReLU->MaxPool, kernel_size를 매우 크게 가져감(11, 11)\n",
    "    input_tensor = Input(shape=in_shape)\n",
    "    \n",
    "    x = Conv2D(filters= 96, kernel_size=(11,11), strides=(4,4), padding='valid')(input_tensor)\n",
    "    x = Activation('relu')(x)\n",
    "    # LRN을 대신하여 Batch Normalization 적용. \n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "\n",
    "\n",
    "    # 두번째 CNN->ReLU->MaxPool. kernel_size=(5, 5)\n",
    "    x = Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding='same',kernel_regularizer=kernel_regular)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "\n",
    "\n",
    "    # 3x3 Conv 2번 연속 적용. filters는 384개\n",
    "    x = Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same', kernel_regularizer=kernel_regular)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same', kernel_regularizer=kernel_regular)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # 3x3 Conv를 적용하되 filters 수를 줄이고 maxpooling을 적용\n",
    "    x = Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same', kernel_regularizer=kernel_regular)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(3,3), strides=(2,2))(x)\n",
    "\n",
    "    # Dense 연결을 위한 Flatten\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # Dense + Dropout을 연속 적용. \n",
    "    x = Dense(units = 4096, activation = 'relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Dense(units = 4096, activation = 'relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # 마지막 softmax 층 적용. \n",
    "    output = Dense(units = n_classes, activation = 'softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input_tensor, outputs=output)\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_alexnet(in_shape=(227, 227, 3), n_classes=10, kernel_regular=regularizers.l2(l2=1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967fb649-b85c-46d2-a39a-0974e38dc4e7",
   "metadata": {},
   "source": [
    "## CIFAR10 데이터세트를 이용하여 AlextNet 학습 및 성능 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8e81a96-db72-444f-9e6a-97da3574434b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset shape: (50000, 32, 32, 3) (50000, 1)\n",
      "test dataset shape: (10000, 32, 32, 3) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# 전체 6만개 데이터 중, 5만개는 학습 데이터용, 1만개는 테스트 데이터용으로 분리\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "print(\"train dataset shape:\", train_images.shape, train_labels.shape)\n",
    "print(\"test dataset shape:\", test_images.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ff9db9-6335-4a6b-9a9e-34a03bd0e7af",
   "metadata": {},
   "source": [
    "## 학습/검증/테스트 데이터 세트로 나누고 데이터 전처리 수행\n",
    "* 학습/검증/테스트 데이터로 분할. 검증 데이터는 학습 데이터의 20% 할당. \n",
    "* 레이블의 원-핫 인코딩과 이미지 픽셀값의 스케일링 적용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79c0214b-bb3e-45ed-b201-cf5f9edd87e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n",
      "(40000, 32, 32, 3) (40000, 10) (10000, 32, 32, 3) (10000, 10) (10000, 32, 32, 3) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random as python_random\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "def zero_one_scaler(image):\n",
    "    return image/255.0\n",
    "\n",
    "def get_preprocessed_ohe(images, labels, pre_func=None):\n",
    "    # preprocessing 함수가 입력되면 이를 이용하여 image array를 scaling 적용.\n",
    "    if pre_func is not None:\n",
    "        images = pre_func(images)\n",
    "    # OHE 적용    \n",
    "    oh_labels = to_categorical(labels)\n",
    "    return images, oh_labels\n",
    "\n",
    "# 학습/검증/테스트 데이터 세트에 전처리 및 OHE 적용한 뒤 반환 \n",
    "def get_train_valid_test_set(train_images, train_labels, test_images, test_labels, valid_size=0.15, random_state=2021):\n",
    "    # 학습 및 테스트 데이터 세트를  0 ~ 1사이값 float32로 변경 및 OHE 적용. \n",
    "    train_images, train_oh_labels = get_preprocessed_ohe(train_images, train_labels)\n",
    "    test_images, test_oh_labels = get_preprocessed_ohe(test_images, test_labels)\n",
    "    \n",
    "    # 학습 데이터를 검증 데이터 세트로 다시 분리\n",
    "    tr_images, val_images, tr_oh_labels, val_oh_labels = train_test_split(train_images, train_oh_labels, test_size=valid_size, random_state=random_state)\n",
    "    \n",
    "    return (tr_images, tr_oh_labels), (val_images, val_oh_labels), (test_images, test_oh_labels )\n",
    "\n",
    "# CIFAR10 데이터 재 로딩 및 Scaling/OHE 전처리 적용하여 학습/검증/데이터 세트 생성. \n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "print(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)\n",
    "(tr_images, tr_oh_labels), (val_images, val_oh_labels), (test_images, test_oh_labels) = \\\n",
    "    get_train_valid_test_set(train_images, train_labels, test_images, test_labels, valid_size=0.2, random_state=2021)\n",
    "\n",
    "print(tr_images.shape, tr_oh_labels.shape, val_images.shape, val_oh_labels.shape, test_images.shape, test_oh_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2c6fcd4-908b-416f-8067-4ea8355c4cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 30, 30, 96)        34944     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 30, 30, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 30, 30, 96)        384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 14, 14, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 14, 256)       614656    \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 6, 6, 384)         885120    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 6, 6, 384)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 6, 6, 384)         1536      \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 6, 6, 384)         1327488   \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 6, 6, 384)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 6, 6, 384)         1536      \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 6, 6, 256)         884992    \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4096)              4198400   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 24,773,386\n",
      "Trainable params: 24,770,634\n",
      "Non-trainable params: 2,752\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 이미지 사이즈가 너무 작으면 모델의 MaxPooling에서 오류 발생. \n",
    "model = create_alexnet(in_shape=(128, 128, 3), n_classes=10, kernel_regular=regularizers.l2(l2=1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4869c00-c827-4062-979b-71e092a72219",
   "metadata": {},
   "source": [
    "### CIFAR10 원본 이미지 크기 32x32 를 128x128로 증가 시키는 Sequence Dataset 생성\n",
    "* 128x128로 CIFAR10 모든 이미지 배열값을 증가시키면 RAM 부족 발생. \n",
    "* 배치 크기 만큼의 개수만 원본 이미지를 128x128로 증가 시킨 뒤(opencv의 resize()), 이를 모델에 입력하는 로직으로 Sequence Dataset 구성. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "170462b4-0162-4621-be24-d9f11498f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 128\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aed9c72-066e-4c20-8563-872d5debe19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "import cv2\n",
    "import sklearn\n",
    "\n",
    "# 입력 인자 images_array labels는 모두 numpy array로 들어옴. \n",
    "# 인자로 입력되는 images_array는 전체 32x32 image array임. \n",
    "class CIFAR_Dataset(Sequence):\n",
    "    def __init__(self, images_array, labels, batch_size=BATCH_SIZE, augmentor=None, shuffle=False, pre_func=None):\n",
    "        '''\n",
    "        파라미터 설명\n",
    "        images_array: 원본 32x32 만큼의 image 배열값. \n",
    "        labels: 해당 image의 label들\n",
    "        batch_size: __getitem__(self, index) 호출 시 마다 가져올 데이터 batch 건수\n",
    "        augmentor: albumentations 객체\n",
    "        shuffle: 학습 데이터의 경우 epoch 종료시마다 데이터를 섞을지 여부\n",
    "        '''\n",
    "        # 객체 생성 인자로 들어온 값을 객체 내부 변수로 할당. \n",
    "        # 인자로 입력되는 images_array는 전체 32x32 image array임.\n",
    "        self.images_array = images_array\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.augmentor = augmentor\n",
    "        self.pre_func = pre_func\n",
    "        # train data의 경우 \n",
    "        self.shuffle = shuffle\n",
    "        if self.shuffle:\n",
    "            # 객체 생성시에 한번 데이터를 섞음. \n",
    "            #self.on_epoch_end()\n",
    "            pass\n",
    "    \n",
    "    # Sequence를 상속받은 Dataset은 batch_size 단위로 입력된 데이터를 처리함. \n",
    "    # __len__()은 전체 데이터 건수가 주어졌을 때 batch_size단위로 몇번 데이터를 반환하는지 나타남\n",
    "    def __len__(self):\n",
    "        # batch_size단위로 데이터를 몇번 가져와야하는지 계산하기 위해 전체 데이터 건수를 batch_size로 나누되, 정수로 정확히 나눠지지 않을 경우 1회를 더한다. \n",
    "        return int(np.ceil(len(self.labels) / self.batch_size))\n",
    "    \n",
    "    # batch_size 단위로 image_array, label_array 데이터를 가져와서 변환한 뒤 다시 반환함\n",
    "    # 인자로 몇번째 batch 인지를 나타내는 index를 입력하면 해당 순서에 해당하는 batch_size 만큼의 데이타를 가공하여 반환\n",
    "    # batch_size 갯수만큼 변환된 image_array와 label_array 반환. \n",
    "    def __getitem__(self, index):\n",
    "        # index는 몇번째 batch인지를 나타냄. \n",
    "        # batch_size만큼 순차적으로 데이터를 가져오려면 array에서 index*self.batch_size:(index+1)*self.batch_size 만큼의 연속 데이터를 가져오면 됨\n",
    "        # 32x32 image array를 self.batch_size만큼 가져옴. \n",
    "        images_fetch = self.images_array[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        if self.labels is not None:\n",
    "            label_batch = self.labels[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        # 만일 객체 생성 인자로 albumentation으로 만든 augmentor가 주어진다면 아래와 같이 augmentor를 이용하여 image 변환\n",
    "        # albumentations은 개별 image만 변환할 수 있으므로 batch_size만큼 할당된 image_name_batch를 한 건씩 iteration하면서 변환 수행. \n",
    "        # 변환된 image 배열값을 담을 image_batch 선언. image_batch 배열은 float32 로 설정. \n",
    "        image_batch = np.zeros((images_fetch.shape[0], IMAGE_SIZE, IMAGE_SIZE, 3), dtype='float32')\n",
    "        \n",
    "        # batch_size에 담긴 건수만큼 iteration 하면서 opencv image load -> image augmentation 변환(augmentor가 not None일 경우)-> image_batch에 담음. \n",
    "        for image_index in range(images_fetch.shape[0]):\n",
    "            #image = cv2.cvtColor(cv2.imread(image_name_batch[image_index]), cv2.COLOR_BGR2RGB)\n",
    "            # 원본 image를 IMAGE_SIZE x IMAGE_SIZE 크기로 변환\n",
    "            image = cv2.resize(images_fetch[image_index], (IMAGE_SIZE, IMAGE_SIZE))\n",
    "            # 만약 augmentor가 주어졌다면 이를 적용. \n",
    "            if self.augmentor is not None:\n",
    "                image = self.augmentor(image=image)['image']\n",
    "                \n",
    "            # 만약 scaling 함수가 입력되었다면 이를 적용하여 scaling 수행. \n",
    "            if self.pre_func is not None:\n",
    "                image = self.pre_func(image)\n",
    "            \n",
    "            # image_batch에 순차적으로 변환된 image를 담음.               \n",
    "            image_batch[image_index] = image\n",
    "        \n",
    "        return image_batch, label_batch\n",
    "    \n",
    "    # epoch가 한번 수행이 완료 될 때마다 모델의 fit()에서 호출됨. \n",
    "    def on_epoch_end(self):\n",
    "        if(self.shuffle):\n",
    "            #print('epoch end')\n",
    "            # 원본 image배열과 label를 쌍을 맞춰서 섞어준다. scikt learn의 utils.shuffle에서 해당 기능 제공\n",
    "            self.images_array, self.labels = sklearn.utils.shuffle(self.images_array, self.labels)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94aff421-0071-4cbd-8381-c21132ec373f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128, 128, 3) (64, 128, 128, 3)\n",
      "(64, 10) (64, 10)\n",
      "[[[0.6431373  0.68235296 0.69411767]\n",
      "  [0.6431373  0.68235296 0.69411767]\n",
      "  [0.63529414 0.67058825 0.68235296]\n",
      "  ...\n",
      "  [0.28627452 0.2901961  0.27058825]\n",
      "  [0.27450982 0.28235295 0.26666668]\n",
      "  [0.27450982 0.28235295 0.26666668]]\n",
      "\n",
      " [[0.6431373  0.68235296 0.69411767]\n",
      "  [0.6431373  0.68235296 0.69411767]\n",
      "  [0.63529414 0.67058825 0.68235296]\n",
      "  ...\n",
      "  [0.28627452 0.2901961  0.27058825]\n",
      "  [0.27450982 0.28235295 0.26666668]\n",
      "  [0.27450982 0.28235295 0.26666668]]\n",
      "\n",
      " [[0.6509804  0.6901961  0.7019608 ]\n",
      "  [0.6509804  0.6901961  0.7019608 ]\n",
      "  [0.6431373  0.6784314  0.6901961 ]\n",
      "  ...\n",
      "  [0.28627452 0.2901961  0.27058825]\n",
      "  [0.2784314  0.28235295 0.26666668]\n",
      "  [0.2784314  0.28235295 0.26666668]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.9137255  0.85882354 0.8784314 ]\n",
      "  [0.9137255  0.85882354 0.8784314 ]\n",
      "  [0.90588236 0.84705883 0.8627451 ]\n",
      "  ...\n",
      "  [0.26666668 0.28235295 0.26666668]\n",
      "  [0.27450982 0.28627452 0.27058825]\n",
      "  [0.27450982 0.28627452 0.27058825]]\n",
      "\n",
      " [[0.91764706 0.8627451  0.88235295]\n",
      "  [0.91764706 0.8627451  0.88235295]\n",
      "  [0.90588236 0.8509804  0.8666667 ]\n",
      "  ...\n",
      "  [0.2627451  0.2784314  0.2627451 ]\n",
      "  [0.27058825 0.28235295 0.26666668]\n",
      "  [0.27058825 0.28235295 0.26666668]]\n",
      "\n",
      " [[0.91764706 0.8627451  0.88235295]\n",
      "  [0.91764706 0.8627451  0.88235295]\n",
      "  [0.90588236 0.8509804  0.8666667 ]\n",
      "  ...\n",
      "  [0.26666668 0.2784314  0.2627451 ]\n",
      "  [0.27058825 0.28235295 0.26666668]\n",
      "  [0.27058825 0.28235295 0.26666668]]]\n"
     ]
    }
   ],
   "source": [
    "def zero_one_scaler(image):\n",
    "    return image/255.0\n",
    "\n",
    "tr_ds = CIFAR_Dataset(tr_images, tr_oh_labels, batch_size=BATCH_SIZE, augmentor=None, shuffle=True, pre_func=zero_one_scaler)\n",
    "val_ds = CIFAR_Dataset(val_images, val_oh_labels, batch_size=BATCH_SIZE, augmentor=None, shuffle=False, pre_func=zero_one_scaler)\n",
    "\n",
    "print(next(iter(tr_ds))[0].shape, next(iter(val_ds))[0].shape)\n",
    "print(next(iter(tr_ds))[1].shape, next(iter(val_ds))[1].shape)\n",
    "print(next(iter(tr_ds))[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76be081-5a69-4acd-9a72-7fcba5440442",
   "metadata": {},
   "source": [
    "## Input 크기가 128x128x3 인 AlexNet 모델을 생성하고 epochs는 30으로 설정하고 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03493a14-6a01-45d9-b454-9f4888be7cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 30, 30, 96)        34944     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 30, 30, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 30, 30, 96)        384       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 14, 14, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 14, 14, 256)       614656    \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 6, 6, 384)         885120    \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 6, 6, 384)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 6, 6, 384)         1536      \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 6, 6, 384)         1327488   \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 6, 6, 384)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 6, 6, 384)         1536      \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 6, 6, 256)         884992    \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4096)              4198400   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 24,773,386\n",
      "Trainable params: 24,770,634\n",
      "Non-trainable params: 2,752\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAY\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 24s 29ms/step - loss: 2.0583 - accuracy: 0.3478 - val_loss: 5.4315 - val_accuracy: 0.1474\n",
      "Epoch 2/30\n",
      "625/625 [==============================] - 17s 28ms/step - loss: 1.5290 - accuracy: 0.5084 - val_loss: 1.4799 - val_accuracy: 0.5226\n",
      "Epoch 3/30\n",
      "625/625 [==============================] - 18s 28ms/step - loss: 1.3262 - accuracy: 0.5928 - val_loss: 1.3780 - val_accuracy: 0.5775\n",
      "Epoch 4/30\n",
      "625/625 [==============================] - 18s 28ms/step - loss: 1.2181 - accuracy: 0.6431 - val_loss: 1.1655 - val_accuracy: 0.6627\n",
      "Epoch 5/30\n",
      "625/625 [==============================] - 18s 28ms/step - loss: 1.1244 - accuracy: 0.6908 - val_loss: 1.4218 - val_accuracy: 0.6019\n",
      "Epoch 6/30\n",
      "625/625 [==============================] - 18s 28ms/step - loss: 1.0511 - accuracy: 0.7309 - val_loss: 1.1316 - val_accuracy: 0.7048\n",
      "Epoch 7/30\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.0086 - accuracy: 0.7577 - val_loss: 1.1349 - val_accuracy: 0.7172\n",
      "Epoch 8/30\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.0808 - accuracy: 0.7448 - val_loss: 1.3503 - val_accuracy: 0.6586\n",
      "Epoch 9/30\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 1.0267 - accuracy: 0.7743 - val_loss: 1.5534 - val_accuracy: 0.6184\n",
      "Epoch 10/30\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.9463 - accuracy: 0.8091 - val_loss: 1.1906 - val_accuracy: 0.7318\n",
      "Epoch 11/30\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 1.0745 - accuracy: 0.7786 - val_loss: 2.4468 - val_accuracy: 0.3191\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 12/30\n",
      "625/625 [==============================] - 20s 32ms/step - loss: 1.2711 - accuracy: 0.7223 - val_loss: 1.2513 - val_accuracy: 0.7243\n",
      "Epoch 13/30\n",
      "625/625 [==============================] - 19s 30ms/step - loss: 0.8481 - accuracy: 0.8557 - val_loss: 1.0386 - val_accuracy: 0.7945\n",
      "Epoch 14/30\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.6635 - accuracy: 0.9083 - val_loss: 1.0572 - val_accuracy: 0.7978\n",
      "Epoch 15/30\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.5544 - accuracy: 0.9380 - val_loss: 1.1345 - val_accuracy: 0.7933\n",
      "Epoch 16/30\n",
      "625/625 [==============================] - 18s 28ms/step - loss: 0.4808 - accuracy: 0.9575 - val_loss: 1.1797 - val_accuracy: 0.7964\n",
      "Epoch 17/30\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.4395 - accuracy: 0.9686 - val_loss: 1.2803 - val_accuracy: 0.8020\n",
      "Epoch 18/30\n",
      "625/625 [==============================] - 18s 29ms/step - loss: 0.4113 - accuracy: 0.9739 - val_loss: 1.3273 - val_accuracy: 0.7976\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 19/30\n",
      "625/625 [==============================] - 18s 28ms/step - loss: 0.3641 - accuracy: 0.9891 - val_loss: 1.3792 - val_accuracy: 0.8143\n",
      "Epoch 20/30\n",
      "625/625 [==============================] - 18s 28ms/step - loss: 0.3449 - accuracy: 0.9939 - val_loss: 1.4646 - val_accuracy: 0.8122\n",
      "Epoch 21/30\n",
      "625/625 [==============================] - 18s 28ms/step - loss: 0.3338 - accuracy: 0.9961 - val_loss: 1.5693 - val_accuracy: 0.8118\n",
      "Epoch 22/30\n",
      "625/625 [==============================] - 18s 28ms/step - loss: 0.3269 - accuracy: 0.9963 - val_loss: 1.6551 - val_accuracy: 0.8133\n",
      "Epoch 23/30\n",
      "625/625 [==============================] - 18s 28ms/step - loss: 0.3229 - accuracy: 0.9966 - val_loss: 1.6750 - val_accuracy: 0.8139\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 00023: early stopping\n"
     ]
    }
   ],
   "source": [
    "model = create_alexnet(in_shape=(128, 128, 3), n_classes=10, kernel_regular=regularizers.l2(l2=1e-4))\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 5번 iteration내에 validation loss가 향상되지 않으면 learning rate을 기존 learning rate * 0.2로 줄임.  \n",
    "rlr_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, mode='min', verbose=1)\n",
    "ely_cb = EarlyStopping(monitor='val_loss', patience=10, mode='min', verbose=1)\n",
    "\n",
    "history = model.fit(tr_ds, epochs=30, \n",
    "                    #steps_per_epoch=int(np.ceil(tr_images.shape[0]/BATCH_SIZE)),\n",
    "                    validation_data=val_ds, \n",
    "                    #validation_steps=int(np.ceil(val_images.shape[0]/BATCH_SIZE)), \n",
    "                    callbacks=[rlr_cb, ely_cb]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b7ac1ae-a8c7-4561-babb-f7503e8f9425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 2s 15ms/step - loss: 1.7500 - accuracy: 0.8020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.75004243850708, 0.8019999861717224]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = CIFAR_Dataset(test_images, test_oh_labels, batch_size=BATCH_SIZE, augmentor=None, shuffle=False, pre_func=zero_one_scaler)\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e12a42-d5de-41c7-ab44-4ffc0367787b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
